{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All about Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature scaling/normalizing\n",
    "a. In scaling, you're changing the range of your data. You want to scale data when you're using methods based on measures of how far apart data points are, like support vector machines (SVM) or k-nearest neighbors (KNN). With these algorithms, a change of \"1\" in any numeric feature is given the same importance.\n",
    "\n",
    "b. In normalization, you're changing the shape of the distribution of your data. In general, you'll normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include linear discriminant analysis (LDA) and Gaussian naive Bayes. (Pro tip: any method with \"Gaussian\" in the name probably assumes normality.)\n",
    "\n",
    "Why features need to scale/normalize?\n",
    "\n",
    "a. Since the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization. For example, many classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.\n",
    "\n",
    "b. Another reason why feature scaling is applied is that gradient descent converges much faster with feature scaling than without it.\n",
    "\n",
    "c. It's also important to apply feature scaling if regularization is used as part of the loss function (so that coefficients are penalized appropriately)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluate model using Cross-Valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input data\n",
    "df = pd.read_csv(\"file.name\")\n",
    "X = df.copy()\n",
    "y = X.pop(\"column.name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model base on Cross-Valuation scores\n",
    "model = RandomForestRegressor(criterion=\"mae\", random_state=0)\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring=\"neg_mean_absolute_error\")\n",
    "scores = -1*scores.mean() # mean of all scores (5 scores in this case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply Mutual Information to measure a relationship between two quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import mutual_info_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The scikit-learn algorithm for MI treats discrete features differently from continuous features. \n",
    "Consequently, you need to tell it which are which. As a rule of thumb, anything that must have a float \n",
    "dtype is not discrete. Categoricals (object or categorial dtype) can be treated as discrete by giving them a label encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calculates MI scores\n",
    "def make_mi_scores(X, y):\n",
    "    X = X.copy()\n",
    "    for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # All discrete features should now have integer dtypes\n",
    "    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting MI scores\n",
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mathematical Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ratio\n",
    "The \"stroke ratio\", for instance, is a measure of how efficient an engine is versus how performant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autos[\"stroke_ratio\"] = autos.stroke / autos.bore\n",
    "autos[[\"stroke\", \"bore\", \"stroke_ratio\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize\n",
    "Data visualization can suggest transformations, often a \"reshaping\" of a feature through powers or logarithms. The distribution of WindSpeed in US Accidents is highly skewed, for instance. In this case the logarithm is effective at normalizing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the feature has 0.0 values, use np.log1p (log(1+x)) instead of np.log\n",
    "accidents[\"LogWindSpeed\"] = accidents.WindSpeed.apply(np.log1p)\n",
    "\n",
    "# Plot a comparison\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "sns.kdeplot(accidents.WindSpeed, shade=True, ax=axs[0])\n",
    "sns.kdeplot(accidents.LogWindSpeed, shade=True, ax=axs[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counts\n",
    "Features describing the presence or absence of something often come in sets, the set of risk factors for a disease, say. You can aggregate such features by creating a count.\n",
    "\n",
    "These features will be binary (1 for Present, 0 for Absent) or boolean (True or False). In Python, booleans can be added up just as if they were integers.\n",
    "\n",
    "In Traffic Accidents are several features indicating whether some roadway object was near the accident. This will create a count of the total number of roadway features nearby using the sum method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roadway_features = [\"Amenity\", \"Bump\", \"Crossing\", \"GiveWay\",\n",
    "    \"Junction\", \"NoExit\", \"Railway\", \"Roundabout\", \"Station\", \"Stop\",\n",
    "    \"TrafficCalming\", \"TrafficSignal\"]\n",
    "accidents[\"RoadwayFeatures\"] = accidents[roadway_features].sum(axis=1) # Sum number of features (True = 1 and False = 0).\n",
    "\n",
    "accidents[roadway_features + [\"RoadwayFeatures\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many formulations lack one or more components (that is, the component has a value of 0). This will count how many components are in a formulation with the dataframe's built-in greater-than gt method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [ \"Cement\", \"BlastFurnaceSlag\", \"FlyAsh\", \"Water\",\n",
    "               \"Superplasticizer\", \"CoarseAggregate\", \"FineAggregate\"]\n",
    "concrete[\"Components\"] = concrete[components].gt(0).sum(axis=1) # Count the components that greater than 0.\n",
    "\n",
    "concrete[components + [\"Components\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building-Up and Breaking-Down Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From the Policy feature, we could separate the Type from the Level of coverage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer[[\"Type\", \"Level\"]] = (  # Create two new features\n",
    "    customer[\"Policy\"]           # from the Policy feature\n",
    "    .str                         # through the string accessor\n",
    "    .split(\" \", expand=True)     # by splitting on \" \"\n",
    "                                 # and expanding the result into separate columns\n",
    ")\n",
    "\n",
    "customer[[\"Policy\", \"Type\", \"Level\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also join simple features into a composed feature if you had reason to believe there was some interaction in the combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autos[\"make_and_style\"] = autos[\"make\"] + \"_\" + autos[\"body_style\"]\n",
    "autos[[\"make\", \"body_style\", \"make_and_style\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert our date columns to datetime:\n",
    "1/17/07 has the format \"%m/%d/%y\"; \n",
    "17-1-2007 has the format \"%d-%m-%Y\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column, date_parsed, with the parsed dates\n",
    "landslides['date_parsed'] = pd.to_datetime(landslides['date'], format=\"%m/%d/%y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Transforms\n",
    "Finally we have Group transforms, which aggregate information across multiple rows grouped by some category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer[\"AverageIncome\"] = (\n",
    "    customer.groupby(\"State\")  # for each state\n",
    "    [\"Income\"]                 # select the income\n",
    "    .transform(\"mean\")         # and compute its mean\n",
    ")\n",
    "\n",
    "customer[[\"State\", \"Income\", \"AverageIncome\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods include max, min, median, var, std, and count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer[\"StateFreq\"] = (\n",
    "    customer.groupby(\"State\") # for each State\n",
    "    [\"State\"]  # Select the state\n",
    "    .transform(\"count\") # count the number of each state\n",
    "    / customer.State.count() # count total number in state column\n",
    ")\n",
    "\n",
    "customer[[\"State\", \"StateFreq\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using training and validation splits, to preserve their independence, it's best to create a grouped feature using only the training set and then join it to the validation set. We can use the validation set's merge method after creating a unique set of values with drop_duplicates on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splits\n",
    "df_train = customer.sample(frac=0.5)\n",
    "df_valid = customer.drop(df_train.index)\n",
    "\n",
    "# Create the average claim amount by coverage type, on the training set\n",
    "df_train[\"AverageClaim\"] = df_train.groupby(\"Coverage\")[\"ClaimAmount\"].transform(\"mean\")\n",
    "\n",
    "# Merge the values into the validation set\n",
    "df_valid = df_valid.merge(\n",
    "    df_train[[\"Coverage\", \"AverageClaim\"]].drop_duplicates(),\n",
    "    on=\"Coverage\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "df_valid[[\"Coverage\", \"AverageClaim\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips on Creating Features\n",
    "It's good to keep in mind your model's own strengths and weaknesses when creating features. Here are some guidelines:\n",
    "1. Linear models learn sums and differences naturally, but can't learn anything more complex.\n",
    "2. Ratios seem to be difficult for most models to learn. Ratio combinations often lead to some easy performance gains.\n",
    "3. Linear models and neural nets generally do better with normalized features. Neural nets especially need features scaled to values not too far from 0. Tree-based models (like random forests and XGBoost) can sometimes benefit from normalization, but usually much less so.\n",
    "4. Tree models can learn to approximate almost any combination of features, but when a combination is especially important they can still benefit from having it explicitly created, especially when data is limited.\n",
    "5. Counts are especially helpful for tree models, since these models don't have a natural way of aggregating information across many features at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
